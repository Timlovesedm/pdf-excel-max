import streamlit as st
import pandas as pd
import pdfplumber
import io
import re
from collections import defaultdict

# ==========================================
# --- ãƒ„ãƒ¼ãƒ«â‘ ï¼šPDFã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡ºã™ã‚‹é–¢æ•° ---
# ==========================================
def extract_tables_from_multiple_pdfs(pdf_files, keywords, global_start, global_end, file_specific_ranges=None):
    """
    pdf_files: ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ãƒªã‚¹ãƒˆ
    keywords: æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒªã‚¹ãƒˆ
    global_start: å…±é€šé–‹å§‹ãƒšãƒ¼ã‚¸ (Noneãªã‚‰æœ€åˆã‹ã‚‰)
    global_end: å…±é€šçµ‚äº†ãƒšãƒ¼ã‚¸ (Noneãªã‚‰æœ€å¾Œã¾ã§)
    file_specific_ranges: { "ãƒ•ã‚¡ã‚¤ãƒ«å": {"start": int, "end": int} } å½¢å¼ã®è¾æ›¸
    """
    all_rows = []
    if not keywords:
        st.error("â— ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒå…¥åŠ›ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚", icon="ğŸš¨")
        return None

    for pdf_file in pdf_files:
        all_rows.append([f"ãƒ•ã‚¡ã‚¤ãƒ«å: {pdf_file.name}"])
        all_rows.append([])
        
        # --- ãƒšãƒ¼ã‚¸ç¯„å›²ã®æ±ºå®šãƒ­ã‚¸ãƒƒã‚¯ ---
        # å€‹åˆ¥è¨­å®šãŒã‚ã‚‹ã‹ç¢ºèª
        current_start = global_start
        current_end = global_end
        
        if file_specific_ranges and pdf_file.name in file_specific_ranges:
            spec = file_specific_ranges[pdf_file.name]
            # å€‹åˆ¥è¨­å®šã§å€¤ãŒå…¥ã£ã¦ã„ã‚Œã°ãã‚Œã‚’æ¡ç”¨ã€ãªã‘ã‚Œã°None(å…¨ç¯„å›²)
            current_start = spec.get("start") 
            current_end = spec.get("end")

        found_in_file = False
        try:
            with pdfplumber.open(pdf_file) as pdf:
                # ãƒšãƒ¼ã‚¸ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®è¨ˆç®— (1å§‹ã¾ã‚Šã‚’0å§‹ã¾ã‚Šã«å¤‰æ›)
                s_idx = (current_start - 1) if current_start else 0
                e_idx = current_end if current_end else len(pdf.pages)
                
                # ç¯„å›²å¤–ã‚¨ãƒ©ãƒ¼å›é¿
                s_idx = max(0, s_idx)
                e_idx = min(len(pdf.pages), e_idx)
                
                if s_idx >= e_idx:
                    st.warning(f"ãƒ•ã‚¡ã‚¤ãƒ«ã€Œ{pdf_file.name}ã€: ãƒšãƒ¼ã‚¸ç¯„å›²æŒ‡å®šãŒç„¡åŠ¹ã§ã™ï¼ˆé–‹å§‹ {current_start} ï½ çµ‚äº† {current_end}ï¼‰ã€‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚", icon="âš ï¸")
                    continue

                target_pages = pdf.pages[s_idx:e_idx]
                
                for page in target_pages:
                    text = page.extract_text() or ""
                    if any(kw in text for kw in keywords):
                        found_in_file = True
                        tables = page.extract_tables()
                        for table_index, table in enumerate(tables):
                            if not table:
                                continue
                            all_rows.append([f"--- ãƒšãƒ¼ã‚¸ {page.page_number} / ãƒ†ãƒ¼ãƒ–ãƒ« {table_index + 1} ---"])
                            for row in table:
                                cleaned_row = ["" if item is None else str(item).replace("\n", " ") for item in row]
                                all_rows.append(cleaned_row)
                            all_rows.append([])
        except Exception as e:
            st.error(f"ãƒ•ã‚¡ã‚¤ãƒ«ã€Œ{pdf_file.name}ã€å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}", icon="ğŸ”¥")
            continue
        
        if not found_in_file:
            st.warning(f"ãƒ•ã‚¡ã‚¤ãƒ«ã€Œ{pdf_file.name}ã€ã§ã¯æŒ‡å®šç¯„å›²å†…ã«ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å«ã‚€è¡¨ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚", icon="âš ï¸")

    if not any(r for r in all_rows if r):
        return None
    return pd.DataFrame(all_rows)


# ==========================================
# --- ãƒ„ãƒ¼ãƒ«â‘¡ï¼šå…±é€šãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ï¼ˆå¼·åŒ–ç‰ˆï¼‰ ---
# ==========================================

def detect_year_header(cell_value):
    """ã‚»ãƒ«å†…ã®æ–‡å­—åˆ—ã‹ã‚‰å¹´æ¬¡ãƒ˜ãƒƒãƒ€ãƒ¼ã‚’æ¤œå‡ºã™ã‚‹"""
    cell_value = str(cell_value).strip()
    
    patterns = [
        # YYYYQ1~4
        (re.compile(r"(20\d{2}Q[1-4])", re.IGNORECASE), lambda m: m.group(1).upper()),
        # (è‡ª 2024å¹´4æœˆ...
        (re.compile(r"\(?è‡ª\s*(\d{4})å¹´(\d{1,2})æœˆ"), lambda m: f"{m.group(1)}/{m.group(2)}"),
        # 2024å¹´3æœˆæœŸ, 2024å¹´3æœˆ ç­‰
        (re.compile(r"(\d{4})å¹´(\d{1,2})æœˆ"), lambda m: f"{m.group(1)}/{m.group(2)}"),
        # 2024å¹´åº¦
        (re.compile(r"(\d{4})å¹´åº¦"), lambda m: f"{m.group(1)}å¹´åº¦"),
        # 24/3 (YY/M) å½¢å¼
        (re.compile(r"^\'?(\d{2})/(\d{1,2})$"), lambda m: f"20{m.group(1)}/{m.group(2)}"),
        # 2024/3 (YYYY/M) å½¢å¼
        (re.compile(r"(\d{4})/(\d{1,2})"), lambda m: f"{m.group(1)}/{m.group(2)}"),
        # ã‚·ãƒ³ãƒ—ãƒ«ãªæ•°å€¤ 2024 or 202403
        (re.compile(r"^20\d{2}(\d{2})?$"), lambda m: m.group(0))
    ]

    for pat, formatter in patterns:
        match = pat.search(cell_value)
        if match:
            return formatter(match)
            
    return None

# ==========================================
# --- ãƒ„ãƒ¼ãƒ«â‘¡ï¼šã€ç¸¦æ–¹å‘ã€‘çµ±åˆãƒ­ã‚¸ãƒƒã‚¯ ---
# ==========================================
def tool2_extract_data_vertical(df_chunk):
    if df_chunk.empty:
        return None, []
    
    year_cells = []
    for r in range(df_chunk.shape[0]):
        for c in range(df_chunk.shape[1]):
            cell_value = str(df_chunk.iat[r, c])
            year_header = detect_year_header(cell_value)
            if year_header:
                year_cells.append({"row": r, "col": c, "year_header": year_header})

    if not year_cells:
        return None, []

    year_cells.sort(key=lambda x: (x["row"], x["col"]))
    processed_years = set()
    
    initial_items = df_chunk[0].astype(str).str.strip().dropna()
    initial_items = initial_items[initial_items != ""]
    is_sonota = initial_items == "ãã®ä»–"
    if is_sonota.any():
        sonota_counts = initial_items.groupby(initial_items).cumcount()
        initial_items.loc[is_sonota] = "ãã®ä»–_temp_" + sonota_counts[is_sonota].astype(str)
    
    all_items_ordered = initial_items.drop_duplicates(keep="first").tolist()
    df_result = pd.DataFrame({"å…±é€šé …ç›®": all_items_ordered})

    for cell in year_cells:
        year_header = cell["year_header"]
        if year_header in processed_years:
            continue
        processed_years.add(year_header)
        val_col = cell["col"]
        
        temp_df = df_chunk.iloc[cell["row"] + 1 :, [0, val_col]].copy()
        temp_df.columns = ["å…±é€šé …ç›®", year_header]
        temp_df["å…±é€šé …ç›®"] = temp_df["å…±é€šé …ç›®"].astype(str).str.strip()
        temp_df = temp_df[temp_df["å…±é€šé …ç›®"] != ""].dropna(subset=["å…±é€šé …ç›®"])
        
        is_sonota = temp_df["å…±é€šé …ç›®"] == "ãã®ä»–"
        if is_sonota.any():
            sonota_counts = temp_df.groupby("å…±é€šé …ç›®").cumcount()
            temp_df.loc[is_sonota, "å…±é€šé …ç›®"] = "ãã®ä»–_temp_" + sonota_counts[is_sonota].astype(str)
            
        temp_df[year_header] = (
            pd.to_numeric(temp_df[year_header].astype(str).str.replace(",", ""), errors='coerce').fillna(0)
        )
        temp_df = temp_df.drop_duplicates(subset=["å…±é€šé …ç›®"], keep="first")
        df_result = pd.merge(df_result, temp_df, on="å…±é€šé …ç›®", how="left")

    return df_result, all_items_ordered

# ==========================================
# --- ãƒ„ãƒ¼ãƒ«â‘¡ï¼šã€æ¨ªæ–¹å‘ã€‘çµ±åˆãƒ­ã‚¸ãƒƒã‚¯ ---
# ==========================================
def tool2_extract_data_horizontal(df_chunk):
    if df_chunk.empty:
        return None, []

    # 1. ç©ºåˆ—ã®å‰Šé™¤
    df_clean = df_chunk.replace(r'^\s*$', pd.NA, regex=True).dropna(axis=1, how='all')
    
    if df_clean.shape[1] < 2:
        return None, []

    df_target = df_clean.fillna("") 

    # 2. å¹´æ¬¡ãƒ˜ãƒƒãƒ€ãƒ¼ã‚’æ¢ã™
    detected_header = None
    header_row_idx = -1
    
    for r in range(min(10, df_target.shape[0])): 
        for c in range(df_target.shape[1]):
            val = df_target.iat[r, c]
            header_cand = detect_year_header(val)
            if header_cand:
                detected_header = header_cand
                header_row_idx = r
                break
        if detected_header:
            break
    
    if not detected_header:
        detected_header = str(df_target.iloc[0, -1]).strip()
        if not detected_header:
            detected_header = "Unknown_Period"

    # 3. ãƒ‡ãƒ¼ã‚¿æŠ½å‡ºï¼ˆä¸€ç•ªå·¦ã®åˆ— ã¨ ä¸€ç•ªå³ã®åˆ—ï¼‰
    temp_df = df_target.iloc[:, [0, -1]].copy()
    temp_df.columns = ["å…±é€šé …ç›®", detected_header]
    
    start_row = header_row_idx + 1 if header_row_idx != -1 else 0
    temp_df = temp_df.iloc[start_row:]
    
    # ã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°
    temp_df["å…±é€šé …ç›®"] = temp_df["å…±é€šé …ç›®"].astype(str).str.strip()
    temp_df = temp_df[temp_df["å…±é€šé …ç›®"] != ""].dropna(subset=["å…±é€šé …ç›®"])
    
    temp_df[detected_header] = (
        pd.to_numeric(temp_df[detected_header].astype(str).str.replace(",", ""), errors='coerce')
    )
    temp_df = temp_df.dropna(subset=[detected_header])

    is_sonota = temp_df["å…±é€šé …ç›®"] == "ãã®ä»–"
    if is_sonota.any():
        sonota_counts = temp_df.groupby("å…±é€šé …ç›®").cumcount()
        temp_df.loc[is_sonota, "å…±é€šé …ç›®"] = "ãã®ä»–_temp_" + sonota_counts[is_sonota].astype(str)

    temp_df = temp_df.groupby("å…±é€šé …ç›®", as_index=False).sum()
    item_list = temp_df["å…±é€šé …ç›®"].tolist()

    return temp_df, item_list


# ==========================================
# --- ãƒ„ãƒ¼ãƒ«â‘¡ï¼šãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ãƒ¡ã‚¤ãƒ³é–¢æ•° ---
# ==========================================
def process_files_and_tables(excel_file, integration_mode):
    try:
        xls = pd.ExcelFile(excel_file)
        sheet_name_to_read = "æŠ½å‡ºçµæœ" if "æŠ½å‡ºçµæœ" in xls.sheet_names else xls.sheet_names[0]
        df_full = pd.read_excel(xls, sheet_name=sheet_name_to_read, header=None)
    except Exception as e:
        st.error(f"Excelãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿å¤±æ•—: {e}")
        return None

    df_full[0] = df_full[0].astype(str)
    file_indices = df_full[df_full[0].str.contains(r"ãƒ•ã‚¡ã‚¤ãƒ«å:", na=False)].index.tolist()
    file_chunks = []
    
    if not file_indices:
        file_chunks.append(df_full)
    else:
        for i in range(len(file_indices)):
            start_idx = file_indices[i]
            end_idx = file_indices[i + 1] if i + 1 < len(file_indices) else len(df_full)
            file_chunks.append(df_full.iloc[start_idx:end_idx].reset_index(drop=True))

    grouped_tables = defaultdict(list)
    master_item_order = defaultdict(list)

    for file_chunk in file_chunks:
        page_indices = file_chunk[file_chunk[0].str.contains(r"--- ãƒšãƒ¼ã‚¸", na=False)].index.tolist()
        table_chunks = []
        last_idx = 0
        
        if not page_indices:
            clean_chunk = file_chunk[
                ~file_chunk[0].str.contains(r"ãƒ•ã‚¡ã‚¤ãƒ«å:|---|^\s*$", na=False, regex=True)
            ].dropna(how="all")
            if not clean_chunk.empty:
                table_chunks.append(clean_chunk)
        else:
            for idx in page_indices:
                chunk = file_chunk.iloc[last_idx:idx]
                if not chunk.empty:
                    table_chunks.append(chunk)
                last_idx = idx
            final_chunk = file_chunk.iloc[last_idx:]
            if not final_chunk.empty:
                table_chunks.append(final_chunk)

        for i, table_chunk in enumerate(table_chunks):
            clean_table_chunk = table_chunk[
                ~table_chunk[0].str.contains(r"ãƒ•ã‚¡ã‚¤ãƒ«å:|---", na=False, regex=True)
            ].dropna(how="all")
            
            if clean_table_chunk.empty:
                continue
            
            if integration_mode == "vertical":
                processed_df, item_order = tool2_extract_data_vertical(clean_table_chunk.reset_index(drop=True))
            else: # horizontal
                processed_df, item_order = tool2_extract_data_horizontal(clean_table_chunk.reset_index(drop=True))

            if processed_df is not None and not processed_df.empty:
                grouped_tables[i].append(processed_df)
                
                current_master_order = master_item_order[i]
                if not current_master_order:
                    master_item_order[i].extend(item_order)
                else:
                    last_known_index = -1
                    for item in item_order:
                        if item in current_master_order:
                            last_known_index = current_master_order.index(item)
                        else:
                            current_master_order.insert(last_known_index + 1, item)
                            last_known_index += 1

    final_summaries = []
    for table_index in sorted(grouped_tables.keys()):
        list_of_dfs = grouped_tables[table_index]
        ordered_items = master_item_order[table_index]
        
        if not list_of_dfs:
            continue
            
        result_df = pd.DataFrame({"å…±é€šé …ç›®": ordered_items})
        
        for df_to_merge in list_of_dfs:
            cols_to_drop = [
                col for col in df_to_merge.columns if col in result_df.columns and col != "å…±é€šé …ç›®"
            ]
            result_df = pd.merge(
                result_df, df_to_merge.drop(columns=cols_to_drop), on="å…±é€šé …ç›®", how="left"
            )
            
        result_df.fillna(0, inplace=True)
        
        def sort_key(col_name):
            s = str(col_name).upper().replace('/', '').replace('Q', '0').replace('å¹´åº¦', '').replace('å¹´', '').replace('æœˆ', '')
            digits = "".join(filter(str.isdigit, s))
            if digits:
                return int(digits.ljust(6, '0'))
            return 99999999

        year_cols = sorted(
            [col for col in result_df.columns if col != "å…±é€šé …ç›®"],
            key=sort_key
        )
        final_cols = ["å…±é€šé …ç›®"] + year_cols
        result_df = result_df[final_cols]
        
        for col in year_cols:
            result_df[col] = pd.to_numeric(result_df[col], errors='coerce').fillna(0).astype(int)
            
        result_df["å…±é€šé …ç›®"] = result_df["å…±é€šé …ç›®"].str.replace(r"_temp_\d+$", "", regex=True)
        
        final_summaries.append(result_df)
        
    return final_summaries


# ==========================================
# --- Streamlit UI ---
# ==========================================
st.set_page_config(page_title="å¤šæ©Ÿèƒ½ãƒ„ãƒ¼ãƒ«", layout="wide")
st.title("ğŸ“„ğŸ“Š å¤šæ©Ÿèƒ½ãƒ„ãƒ¼ãƒ«")

# --- ãƒ„ãƒ¼ãƒ«â‘  ---
with st.container(border=True):
    st.header("ãƒ„ãƒ¼ãƒ«â‘ ï¼šPDFè¡¨ãƒ‡ãƒ¼ã‚¿æŠ½å‡º")
    pdf_files = st.file_uploader(
        "PDFãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ï¼ˆè¤‡æ•°å¯ï¼‰", type="pdf", accept_multiple_files=True
    )
    keyword_input_str = st.text_input("æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šï¼‰")
    
    st.subheader("ãƒšãƒ¼ã‚¸ç¯„å›²è¨­å®š")
    # è¨­å®šãƒ¢ãƒ¼ãƒ‰ã®é¸æŠ
    range_mode = st.radio(
        "ç¯„å›²è¨­å®šãƒ¢ãƒ¼ãƒ‰", 
        ("å…¨ã¦ã®ãƒ•ã‚¡ã‚¤ãƒ«ã§åŒã˜ç¯„å›²ã«ã™ã‚‹", "ãƒ•ã‚¡ã‚¤ãƒ«ã”ã¨ã«ç¯„å›²ã‚’æŒ‡å®šã™ã‚‹"),
        index=0
    )
    
    global_start = None
    global_end = None
    file_specific_ranges = {}

    if range_mode == "å…¨ã¦ã®ãƒ•ã‚¡ã‚¤ãƒ«ã§åŒã˜ç¯„å›²ã«ã™ã‚‹":
        col1, col2 = st.columns(2)
        s_in = col1.text_input("é–‹å§‹ãƒšãƒ¼ã‚¸ (å…±é€š)", placeholder="ä¾‹: 5")
        e_in = col2.text_input("çµ‚äº†ãƒšãƒ¼ã‚¸ (å…±é€š)", placeholder="ä¾‹: 10")
        if s_in.isdigit(): global_start = int(s_in)
        if e_in.isdigit(): global_end = int(e_in)
        
    else:
        st.info("å„ãƒ•ã‚¡ã‚¤ãƒ«ã®é–‹å§‹ãƒ»çµ‚äº†ãƒšãƒ¼ã‚¸ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ï¼ˆç©ºæ¬„ã®å ´åˆã¯å…¨ãƒšãƒ¼ã‚¸ãŒå¯¾è±¡ã«ãªã‚Šã¾ã™ï¼‰")
        if pdf_files:
            for i, f in enumerate(pdf_files):
                c1, c2, c3 = st.columns([4, 1, 1])
                c1.write(f"ğŸ“„ **{f.name}**")
                s_in = c2.text_input("é–‹å§‹", key=f"start_{i}_{f.name}", placeholder="1")
                e_in = c3.text_input("çµ‚äº†", key=f"end_{i}_{f.name}", placeholder="Last")
                
                s_val = int(s_in) if s_in.isdigit() else None
                e_val = int(e_in) if e_in.isdigit() else None
                
                # è¾æ›¸ã«ä¿å­˜
                file_specific_ranges[f.name] = {"start": s_val, "end": e_val}
        else:
            st.warning("ã¾ãšã¯ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚")

    if st.button("æŠ½å‡ºé–‹å§‹ â–¶ï¸"):
        if pdf_files:
            keywords = [kw.strip() for kw in keyword_input_str.split(",") if kw.strip()]
            
            with st.spinner("PDFè§£æä¸­..."):
                # ä»¥å‰ã®å¼•æ•° start_page, end_page ã®ä»£ã‚ã‚Šã« global_start, global_end ã¨ specific_ranges ã‚’æ¸¡ã™
                df_result = extract_tables_from_multiple_pdfs(
                    pdf_files, keywords, 
                    global_start=global_start, 
                    global_end=global_end,
                    file_specific_ranges=file_specific_ranges
                )
                
                if df_result is not None and not df_result.empty:
                    st.success("æŠ½å‡ºå®Œäº†ï¼", icon="âœ…")
                    st.dataframe(df_result)
                    output = io.BytesIO()
                    with pd.ExcelWriter(output, engine="xlsxwriter") as writer:
                        df_result.to_excel(writer, index=False, header=False, sheet_name="æŠ½å‡ºçµæœ")
                        workbook = writer.book
                        worksheet = writer.sheets["æŠ½å‡ºçµæœ"]
                        bold_format = workbook.add_format({"bold": True, "font_size": 20})
                        for idx, val in enumerate(df_result[0]):
                            if isinstance(val, str) and val.startswith("ãƒ•ã‚¡ã‚¤ãƒ«å:"):
                                worksheet.set_row(idx, None, bold_format)
                    
                    if keywords:
                        base_name = '_'.join(keywords)
                        download_filename = f"{base_name}_ã¾ã¨ã‚.xlsx"
                    else:
                        download_filename = "æŠ½å‡ºçµæœ_ã¾ã¨ã‚.xlsx"

                    st.download_button(
                        label="ğŸ“¥ Excelãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                        data=output.getvalue(),
                        file_name=download_filename,
                        mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                    )
        else:
            st.error("PDFãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚", icon="ğŸš¨")

st.divider()

# --- ãƒ„ãƒ¼ãƒ«â‘¡ ---
with st.container(border=True):
    st.header("ãƒ„ãƒ¼ãƒ«â‘¡ï¼šçµ±åˆãƒ‡ãƒ¼ã‚¿ä½œæˆ")
    
    st.info("ğŸ“ ãƒ‡ãƒ¼ã‚¿ã®ä¸¦ã³æ–¹ã‚’é¸æŠã—ã¦ãã ã•ã„")
    integration_mode_label = st.radio(
        "çµ±åˆãƒ¢ãƒ¼ãƒ‰é¸æŠ",
        ("ç¸¦æ–¹å‘çµ±åˆ (å¾“æ¥ã®å½¢å¼)", "æ¨ªæ–¹å‘çµ±åˆ (é …ç›®:å·¦ / æ•°å€¤:å³)"),
        help="ãƒ‡ãƒ¼ã‚¿ãŒç¸¦ã«ç©ã¿ä¸ŠãŒã£ã¦ã„ã‚‹å ´åˆã¯ã€Œç¸¦æ–¹å‘ã€ã€æ¨ªä¸¦ã³ã®å¹´æ¬¡ãƒ‡ãƒ¼ã‚¿ã‚’çµåˆã™ã‚‹å ´åˆã¯ã€Œæ¨ªæ–¹å‘ã€ã‚’é¸æŠã—ã¦ãã ã•ã„"
    )
    integration_mode = "vertical" if "ç¸¦æ–¹å‘" in integration_mode_label else "horizontal"
    
    excel_file = st.file_uploader("Excelãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰", type=["xlsx"])
    
    if st.button("çµ±åˆã¾ã¨ã‚è¡¨ã‚’ä½œæˆ â–¶ï¸", disabled=(excel_file is None)):
        with st.spinner("ãƒ‡ãƒ¼ã‚¿æ•´ç†ä¸­..."):
            all_summaries = process_files_and_tables(excel_file, integration_mode)
            
            if all_summaries:
                st.success(f"{len(all_summaries)}å€‹ã®ã¾ã¨ã‚è¡¨ã‚’ä½œæˆï¼", icon="âœ…")
                output_excel = io.BytesIO()
                with pd.ExcelWriter(output_excel, engine="xlsxwriter") as writer:
                    for i, summary_df in enumerate(all_summaries):
                        sheet_name = f"çµ±åˆã¾ã¨ã‚è¡¨_{i+1}"
                        summary_df.to_excel(writer, sheet_name=sheet_name, index=False)
                        worksheet = writer.sheets[sheet_name]
                        worksheet.set_column(0, 0, 30)

                base_name_input = excel_file.name.rsplit('.xlsx', 1)[0]
                mode_suffix = "_ç¸¦çµ±åˆ" if integration_mode == "vertical" else "_æ¨ªçµ±åˆ"
                if base_name_input.endswith('_ã¾ã¨ã‚'):
                    base_name_output = base_name_input.removesuffix('_ã¾ã¨ã‚') + mode_suffix
                else:
                    base_name_output = base_name_input + mode_suffix
                download_filename = f"{base_name_output}.xlsx"

                st.download_button(
                    label="ğŸ“¥ çµ±åˆã¾ã¨ã‚è¡¨ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                    data=output_excel.getvalue(),
                    file_name=download_filename,
                    mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                )
            else:
                st.warning("æœ‰åŠ¹ãªãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚ï¼ˆãƒ˜ãƒƒãƒ€ãƒ¼æœªæ¤œå‡ºã€ã¾ãŸã¯ç©ºåˆ—ã®å•é¡Œã®å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ï¼‰", icon="âš ï¸")
